---
title: "Introduction to counterfactuals"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Introduction to counterfactuals}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

In the following, we explain the `counterfactuals` workflow for both a classification and a regression task using
concrete use cases.

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  fig.width = 7,
  fig.height = 3,
  comment = "#>"
)
options(width = 200)
```

```{r setup, message=FALSE}
library(counterfactuals)
library(iml)
library(randomForest)
```


## Classification Tasks

To illustrate the `counterfactuals` workflow for classification tasks, we search for counterfactuals for 
diabetes-tested patients with `MOC` (Dandl et al. 2020). 

### Data: Pima Indians Diabetes Database

As training data, we use the Pima Indians Diabetes Database from the `mlbench` package.
The data set contains 768 observations with 8 features and the binary target variable `diabetes`.

```{r, echo=FALSE}
column_descr = data.frame(
  rbind(
    cbind("pregnant", "Number of times pregnant"),
    cbind("glucose", "Plasma glucose concentration (glucose tolerance test)"),
    cbind("pressure", "Diastolic blood pressure (mm Hg)"),
    cbind("triceps", "Triceps skin fold thickness (mm)"),
    cbind("insulin", "2-Hour serum insulin (mu U/ml)"),
    cbind("mass", "Body mass index"),
    cbind("pedigree", "Diabetes pedigree function"),
    cbind("age", "Age (years)"),
    cbind("diabetes", "Class variable (test for diabetes)")
  )
)
names(column_descr) <- c("Variable", "Description")
knitr::kable(column_descr, escape = FALSE, format = "html", table.attr = "style='width:100%;'")
```

```{r}
data(PimaIndiansDiabetes, package = "mlbench")  
```


We convert `integerish` features to the `integer` data type to ensure that the counterfactuals will
only contain `integer` values for these features (for example no `2.76` pregnancies).

```{r}
PimaIndiansDiabetes$pregnant = as.integer(PimaIndiansDiabetes$pregnant)
PimaIndiansDiabetes$glucose = as.integer(PimaIndiansDiabetes$glucose)
PimaIndiansDiabetes$age = as.integer(PimaIndiansDiabetes$age)
```


### Fitting a model

First, we train a model to predict `diabetes`, omitting one observation from the training data, which is `x_interest`.

```{r}
set.seed(20210816)
rf = randomForest(diabetes ~ ., data = PimaIndiansDiabetes[-499L, ])
```

### Setting up an iml::Predictor() object

An [`iml::Predictor`](https://christophm.github.io/iml/reference/Predictor.html) object serves as a wrapper for different 
model types. It contains the model and the data for its analysis.

```{r}
predictor = Predictor$new(rf, type = "prob")
```


### Find counterfactuals

For `x_interest`, the model predicts a diabetes probability of 63.4%.

```{r}
x_interest = PimaIndiansDiabetes[499L, ]
predictor$predict(x_interest)
```

Now, I examine which risk factors need to be changed to reduce the predicted diabetes probability to a maximum of 40%.\
Since we want to apply MOC to a classification model, we initialize a `MOCClassif` object.
Individuals whose prediction is farther away from the desired prediction than `epsilon` can be penalized.
Here, we set `epsilon = 0`, penalizing all individuals whose prediction is outside the desired interval. 
With the `fixed_features` argument, we can fix non-actionable features to their current value.

```{r, eval=FALSE}
moc_classif = MOCClassif$new(
  predictor, epsilon = 0, fixed_features = c("pregnant", "age")
)
```

Then, we use the `find_counterfactuals()` method to find counterfactuals for `x_interest`. 
As we aim to find counterfactuals with a predicted diabetes probability of at most 40%, we set the `desired_class` to 
`"pos"` and the `desired_prob` to `c(0, 0.4)`; in the binary classification case, this is equivalent to setting 
`desired_class` to `"neg"` and `desired_prob` to `c(0.6, 1)`.

```{r, eval=FALSE}
cfactuals = moc_classif$find_counterfactuals(
  x_interest, desired_class = "pos", desired_prob = c(0, 0.4)
)
```

```{r, echo=FALSE, results='hide', message=FALSE}
if (!file.exists("introduction-res/cfactuals.RDS")) {
  moc_classif = MOCClassif$new(predictor, epsilon = 0, fixed_features = c("pregnant", "age"))
  cfactuals = moc_classif$find_counterfactuals(x_interest, desired_class = "pos", desired_prob = c(0, 0.4))
  saveRDS(moc_classif, "introduction-res/moc_classif.RDS")
  saveRDS(cfactuals, "introduction-res/cfactuals.RDS")
}
moc_classif = readRDS("introduction-res/moc_classif.RDS")
cfactuals = readRDS("introduction-res/cfactuals.RDS")
```


### The counterfactuals object

The resulting `Counterfactuals` object holds the counterfactuals in the `data` field and possesses several methods for their 
evaluation and visualization.

```{r}
class(cfactuals)
```

Printing a `Counterfactuals` object, gives an overview of the results.
```{r}
print(cfactuals)
```

The `predict()` method returns the predictions for the counterfactuals.

```{r}
head(cbind(cfactuals$data, cfactuals$predict()), 5L)
```

The `evaluate()` method returns the counterfactuals along with the evaluation measures `dist_x_interest`, `dist_target`,
`no_changed`, and `dist_train`. \
Setting the `show_diff` argument to `TRUE` displays the counterfactuals as their difference
to `x_interest`: for a numeric feature, positive values indicate an increase compared to the feature value in `x_interest`
and negative values indicate a decrease; for factors, the counterfactual feature value is displayed if it 
differs from `x_interest.`; `NA` means "no difference" in both cases.

```{r}
head(cfactuals$evaluate(show_diff = TRUE), 5L)
```

The `plot_freq_of_feature_changes()` method plots the frequency of feature changes across all counterfactuals. \
Setting `subset_zero = TRUE` removes all unchanged features from the plot.

```{r, fig.height=2}
cfactuals$plot_freq_of_feature_changes(subset_zero = TRUE)
```

The parallel plot connects the (scaled) feature values of each counterfactual and highlights `x_interest` in blue.

```{r, message=FALSE, fig.height=2.5}
cfactuals$plot_parallel()
```

The white dot in the prediction surface plot represents `x_interest`. All counterfactuals that differ from `x_interest` only in the 
selected features are displayed as black dots. The tick marks next to the axes indicate the marginal distribution of the
counterfactuals.


```{r, fig.height=3}
cfactuals$plot_surface(feature_names = c("mass", "glucose"))
```


### MOC diagnostics

To evaluate the estimated Pareto front, Dandl et al. (2020) use a hypervolume indicator (Zitzler and Thiele 1998) with a 
reference point that represents the maximal values of the objectives.
The evolution of the hypervolume indicator can be plotted together with the evolution of mean and minimum objective values 
using the `plot_statistics()` method.

```{r, fig.height=2.5, results='asis', eval=FALSE}
moc_classif$plot_statistics()
```

```{r, echo=FALSE, eval=FALSE}
sp = moc_classif$plot_statistics()
p = cowplot::plot_grid(sp[[1]], sp[[2]], sp[[3]], nrow = 3)
ggplot2::ggsave("introduction-res/moc_stats_plots.png", p, width = 9, height = 6)
```

```{r, echo=FALSE, out.width='97%'}
knitr::include_graphics("introduction-res/moc_stats_plots.png")
```

Ideally, one would like the mean value of each objective to decrease over the generations, leading to an increase of the
hypervolume.
However, there is often a trade-off between the objectives. Here, the mean values of `dist_target` and `dist_train` remain 
relatively constant over the generations, indicating that it is difficult to minimize both objectives simultaneously.

This trade-off can also be seen in the scatter plot---created by the `plot_search()` method---that visualizes two selected 
objective values of all individuals. Ideally, one would like to have a point shift to the lower left corner over the generations, 
which implies better objective values. Here, the points for the objectives `dist_train` and `dist_target` are shifted to
a middle region, which underlines the difficulty of minimizing both objectives simultaneously.

```{r, fig.height=2.5}
moc_classif$plot_search(objectives = c("dist_train", "dist_target"))
```

The exact interdependence between the objectives depends on the task at hand.

## Regression Tasks

Finding counterfactuals for regression models is analogous to classification models. In this example, we use 
`WhatIf` (Wexler et al. 2019) to search for counterfactuals for housing prices.

### Data: Boston Housing Data

As training data, we use the Boston Housing dataset from the `mlbench` package. 
The dataset contains 506 observations with 13 features and the (continuous) target variable `medv`.

```{r, echo=FALSE}
# data(BostonHousing, package = "mlbench")  
# df = data.frame(
#   Variable = colnames(BostonHousing),
#   Description = c(
#     "Per capita crime rate by town",
#     "Proportion of residential land zoned for lots over 25,000 sq.ft.",
#     "Proportion of non-retail business acres per town",
#     "Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)",
#     "Nitrogen oxides concentration (parts per 10 million).",
#     "Average number of rooms per dwelling.",
#     "Proportion of owner-occupied units built prior to 1940",
#     "Weighted mean of distances to five Boston employment centres.",
#     "Index of accessibility to radial highways.",
#     "Full-value property-tax rate per $10,000.",
#     "Pupil-teacher ratio by town.",
#     "1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town.",
#     "Percentage of lower status of the population",
#     "Median value of owner-occupied homes in USD 1000's"
#   )
# )
# knitr::kable(df, format = "html", table.attr = "style='width:100%;'")
```

```{r}
data(BostonHousing, package = "mlbench")  
```


### Fitting a model

First, we train a model to predict `medv`, again omitting `x_interest` from the training data.

```{r}
set.seed(20210713)
rf = randomForest(medv ~ ., data = BostonHousing[-1L, ])
```

### Setting up an iml::Predictor() object

Then, we initialize an [`iml::Predictor`](https://christophm.github.io/iml/reference/Predictor.html) object.

```{r}
predictor = Predictor$new(rf)
```


### Find counterfactuals

For `x_interest`, the model predicts a median housing value of 28.32.

```{r}
x_interest = BostonHousing[1L, ]
predictor$predict(x_interest)
```

Since we want to apply `WhatIf` to a regression model, we initialize a `WhatIfRegr` object. The argument `n_counterfactuals` 
specifies the number of counterfactuals to return.

```{r}
whatif_regr = WhatIfRegr$new(predictor, n_counterfactuals = 5L)
```

Then, we use the `find_counterfactuals()` method to find counterfactuals for `x_interest` with a predicted
housing value in the interval [30, 32].
 
```{r}
cfactuals = whatif_regr$find_counterfactuals(x_interest, desired_outcome = c(30, 32))
```

### The counterfactuals object

As a result, we obtain a `Counterfactuals` object, just like for the classification task.

```{r}
cfactuals
```


# References

Dandl, Susanne, Christoph Molnar, Martin Binder, and Bernd Bischl. 2020. “MultiObjective Counterfactual Explanations.” In Parallel Problem Solving from Nature – PPSN XVI, edited by Thomas Bäck, Mike Preuss, André Deutz, Hao Wang, Carola Doerr, Michael Emmerich, and Heike Trautmann, 448–69. Cham: Springer International Publishing.

Wexler, James, Mahima Pushkarna, Tolga Bolukbasi, Martin Wattenberg, Fernanda Viégas, and Jimbo Wilson. 2019. “The What-If Tool: Interactive Probing of Machine Learning Models.” IEEE Transactions on Visualization and Computer Graphics 26 (1): 56–65.

Zitzler, Eckart, and Lothar Thiele. 1998. “Multiobjective Optimization Using Evolutionary Algorithms—a Comparative Case Study.” In International Conference on Parallel Problem Solving from Nature, 292–301. Springer.
